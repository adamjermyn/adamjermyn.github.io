<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		
		<meta name="author" content="Adam Jermyn">
		<meta name="description" content="Site Description">
		<meta name="generator" content="Hugo 0.92.2" />
		<title>ELK Proposal - Make the Reporter care about the Predictor’s beliefs &middot; Adam Jermyn</title>
		<link rel="shortcut icon" href="https://adamjermyn.com/images/favicon.ico">
		<link rel="stylesheet" href="https://adamjermyn.com/css/style.css">
		<link rel="stylesheet" href="https://adamjermyn.com/css/highlight.css">

		
		<link rel="stylesheet" href="https://adamjermyn.com/css/font-awesome.min.css">
		

		
		<link href="https://adamjermyn.com/index.xml" rel="alternate" type="application/rss+xml" title="Adam Jermyn" />
		

		

                
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-186473758-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
		
                <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


	</head>

    <body>
       <nav class="main-nav">
	

        <a href='https://adamjermyn.com/'>About</a>
	<a href='https://adamjermyn.com/posts'>Posts</a>
	<a href='https://adamjermyn.com/curated'>Curated</a>
	<a href='https://adamjermyn.com/tags'>Tags</a>
        <a href='https://adamjermyn.com/software'>Software</a>
        <a href='https://adamjermyn.com/workflow'>Workflow</a>
        <a href='https://adamjermyn.com/bookshelf'>Bookshelf</a>

	

</nav>

        <section id="wrapper" class="post">
            <article>
                <header>
                    <h1>
                        ELK Proposal - Make the Reporter care about the Predictor’s beliefs
                    </h1>
                    <h2 class="headline">
		    
                     Jun 13, 2022 09:27 
                    · 1574 words
                    · 8 minute read
                      <span class="tags">
                      
                      
                          
                              <a href="https://adamjermyn.com/tags/ai-safety">AI Safety</a>
                          
                              <a href="https://adamjermyn.com/tags/ai">AI</a>
                          
                              <a href="https://adamjermyn.com/tags/research">research</a>
                          
                      
                      
                      </span>
                    </h2>
                </header>
                
                <section id="post-body">
                    <p><em>Posted to the AI Alignment Forum</em> <a href="https://www.alignmentforum.org/posts/XGbWaA3gbDphajMHm/elk-proposal-make-the-reporter-care-about-the-predictor-s">here</a><em>.</em></p>
<p><em>(This proposal received an honorable mention in the <a href="https://www.notion.so/ELK-Proposal-Make-the-Reporter-care-about-the-Predictor-s-beliefs-4cb30e79267e4d5591ee7161169170c2">ELK prize results</a>, and we believe was classified among strategies which “reward reporters that are sensitive to what’s actually happening in the world”. We do not think that the counterexample to that class of strategies works against our proposal, though, and we have explained why in a note at the end. Feedback, disagreement, and new failure modes are very welcome!)</em></p>
<h3 id="basic-idea">Basic idea</h3>
<p>A Human Simulator only cares about the observations that the human sees and how the human interprets those observations, not the predictor’s understanding of the vault. The Truthful Reporter, by contrast, cares about the predictor’s understanding of the vault, accessed via the posterior distribution returned by the predictor.</p>
<p>We propose a regularizer which favours having the Reporter depend on the Predictor’s posterior distribution <strong>conditioned on the observations</strong> shown to the human. For example, a Reporter that doesn’t look at the Predictor except to simulate what the human would see would be disfavoured.</p>
<h3 id="how-could-we-implement-this-basic-idea-concretely">How could we implement this basic idea concretely?</h3>
<p>Below we provide a specific instantiation of this regularizer. In brief, this is a new loss term which depends on:</p>
<ol>
<li>[answer_gradient] The gradient of the Reporter’s distribution over answers, taken with respect to the Predictor’s posterior distribution. This describes how the Predictor’s posterior should change to maximally change the Reporter’s answers to the Human’s questions.</li>
<li>[observation_gradient] The gradient of the distribution of predicted observations with respect to the predictor’s posterior distribution. This describes how the Predictor’s posterior should change to maximally change the Extractor’s output.</li>
</ol>
<p>Our loss function favours models which have the answer_gradient be ‘further’ from the observation_gradient. We implement this by finding the linear vector space spanned by the components of the observation_gradient, projecting the answer_gradient <strong>out of</strong> that space, and then taking the norm of what’s left.</p>
<h3 id="more-precise-detail-on-strategy">More precise detail on strategy</h3>
<p>The key changes we have made to the pseudocode below are:</p>
<ul>
<li>Added regularizer described above.</li>
<li>Added a method for computing gradients of other functions.</li>
<li>Added a method for constructing a projection operator.</li>
<li>Changed the observation and reporter functions to depend on the predictor’s posterior rather than a sample from that posterior.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">## New auxiliary functions that we need to do some basic calculus</span>
<span style="color:#75715e">## and linear algebra</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gradient</span>(function, indep_vars, <span style="color:#f92672">*</span>args):
	<span style="color:#75715e"># Returns the gradient of the function with respect to indep_vars</span>
	<span style="color:#75715e"># args are passed to the function</span>
	<span style="color:#75715e"># Return value has shape (shape(function), len(indep_vars))</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">projection</span>(answer_gradient):
	<span style="color:#75715e"># Returns the minimal linear projection Proj operator of shape </span>
	<span style="color:#75715e"># (len(posterior),len(posterior)) such that the matrix</span>
	<span style="color:#75715e"># product of Proj and answer_gradient vanishes.</span>

<span style="color:#75715e">## Procedure begins here</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">prediction</span>(before, action, θ):
	<span style="color:#75715e"># returns an autoregressive model for p(z|before, action)</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">posterior</span>(before, action, after, θ):
	<span style="color:#75715e"># returns an autoregressive model for p(z|before, action, after)</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">observation</span>(posterior, θ):
	<span style="color:#75715e"># returns an autoregressive model for p(after|posterior)</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sample_data</span>():
	<span style="color:#75715e"># returns a random (before, action, after) triple from the dataset</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss</span>(θ):
	before, action, after <span style="color:#f92672">=</span> sample_data()
	z_prior <span style="color:#f92672">=</span> prediction(before, action, θ)
	z_posterior <span style="color:#f92672">=</span> posterior(before, action, after, θ)
	kl <span style="color:#f92672">=</span> z_prior<span style="color:#f92672">.</span>kl_divergence(z_posterior)
	logprob <span style="color:#f92672">=</span> observation(z_prior<span style="color:#f92672">.</span>sample(), θ)<span style="color:#f92672">.</span>logp(after)
	<span style="color:#66d9ef">return</span> kl <span style="color:#f92672">-</span> logprob

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Human</span>:
	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">pose_question</span>(before, action, after):
		<span style="color:#75715e"># returns a question</span>
		<span style="color:#75715e"># should be one a human thinks can be answered unambiguously</span>

	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_for_answer</span>(before, action, after, question, answer):
		<span style="color:#75715e"># returns a non-negative loss</span>
		<span style="color:#75715e"># good answers get a loss of 0</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">reporter</span>(question, posterior, θ_reporter):
	<span style="color:#75715e"># Returns a distribution of answers to the question in the world described by posterior</span>

<span style="color:#75715e"># hyperparameter controlling regularization strength</span>
<span style="color:#66d9ef">lambda</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">regularizer</span>(question, z, θ, θ_reporter):
	before, action, after <span style="color:#f92672">=</span> sample_data()
	post <span style="color:#f92672">=</span> posterior(before, action, after, θ) <span style="color:#75715e"># posterior distribution</span>

	<span style="color:#75715e"># answer_gradient is a matrix of shape (len(answer), len(post)</span>
	answer_gradient <span style="color:#f92672">=</span> gradient(reporter, post, (question, post, θ_reporter))

	<span style="color:#75715e"># observation_gradient is a matrix of shape (len(observation), len(post))</span>
	observation_gradient <span style="color:#f92672">=</span> gradient(observation, post, (post, θ))

	observation_projector <span style="color:#f92672">=</span> projection(observation_gradient)

	<span style="color:#75715e"># Favours predictors whose answers depend more on the posterior of the predictor *conditioned on* the observations.</span>
	<span style="color:#66d9ef">return</span> l2_norm(answer_gradient) <span style="color:#f92672">/</span>
	l2_norm(matrix_product(observation_projector, answer_gradient))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">reporter_loss</span>(human, θ, θ_reporter):
	before, action, after <span style="color:#f92672">=</span> sample_data()
	question <span style="color:#f92672">=</span> human<span style="color:#f92672">.</span>pose_question(before, action, after)
	z <span style="color:#f92672">=</span> posterior(before, action, after, θ)<span style="color:#f92672">.</span>sample()
	answer <span style="color:#f92672">=</span> reporter(question, z, θ_reporter)<span style="color:#f92672">.</span>sample()
	loss <span style="color:#f92672">=</span> human<span style="color:#f92672">.</span>loss_for_answer(before, action, after, question, answer)
	<span style="color:#66d9ef">return</span> loss <span style="color:#f92672">+</span> <span style="color:#66d9ef">lambda</span> <span style="color:#f92672">*</span> regularizer(question, z, θ, θ_reporter)
</code></pre></div><h3 id="why-might-this-idea-work">Why might this idea work?</h3>
<p>Consider the case where we have severe ontology mismatch between a very powerful Predictor and a simple Human. A Reporter might simply learn to model the human in this case, and answer what it predicts the Human would think. In this case, the answers that the Reporter depend primarily on what can be inferred from the observations that the human sees. In our framework, this means that the components of the answer_gradient are contained in the span of the components of the observation_gradient, so our projection operation leaves nothing behind. Thus, the regularization term will be very large and the Human Simulator should be heavily disfavoured.</p>
<p>In contrast, the more truthful the Reporter is, the more the answers depend on elements of the predictor’s understanding which are <em>not</em> captured by the observations. This leaves a larger amount of answer_gradient after projection, which is favoured by the regularizer.</p>
<h3 id="how-does-this-address-earlier-counterexamples">How does this address earlier counterexamples?</h3>
<p>Going through the counterexample features on the Prize post on <a href="https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals">LessWrong</a>:</p>
<ul>
<li>Our approach does not rely on matching ontologies. (1)</li>
<li>Our approach does not rely on the size or sophistication of the human or predictor Bayes nets. (2,3,6)</li>
<li>We directly penalize strategies which simulate humans, so it doesn’t matter if it’s easy for <em>the Reporter to simulate humans</em>. (4) We elaborate on this point in our counterexample.</li>
<li>It does not matter for our case if ‘what the human will believe’ is easier for the Reporter to understand/calculate than ‘what is actually true’ because the regularizer disfavors Reporters which only depend on the observations the human sees. (5)</li>
<li>Our approach does not rely on the sequence of actions. (7,8,9)</li>
</ul>
<p>And then in ‘<a href="https://www.lesswrong.com/posts/FnZws8NuKw6BJzmvZ/counterexamples-to-some-elk-proposals#Strategy__Hold_out_sensors">Counterexamples to some ELK proposals</a>’:</p>
<ul>
<li>Our approach does not rely on hold out or hypothetical sensors.</li>
<li>Our approach does not attempt to conduct new experiments or detect covert tampering by analogy.</li>
</ul>
<h2 id="counterexample">Counterexample</h2>
<p>An existing counterexample considers the case that the Reporter learns a Human Simulator. We believe our regularizer addresses this counterexample, but we think the following is a new counterexample.</p>
<p>While it seems less likely that the Predictor would learn a Human Simulator, since it was not trained with the same feedback, it seems possible that a very sophisticated Predictor could learn everything about the Human that it would need in order to perform human simulation. If this happens, then the Reporter could find the Human Simulator within the Predictor and base its answers on that, rather than the predicted state of the vault.</p>
<p>Our regularizer will still penalize some versions of this counterexample. If the Human Simulator inside the Predictor is somehow used by the parts of the Predictor’s posterior that are used by the Extractor to generate observations, our regularizer will (appropriately) disfavour a Reporter that looks at this Predictor’s Human Simulator. For example, the Predictor might be modeling the Human as part of the process of producing the observations (“What if the Human is the one to invade the vault?”).</p>
<p>Some versions of this won’t be caught though. It is possible for the Human Simulator inside the Predictor to be independent of the observations. This can happen if the Human Simulator is expressing the answer to the class of <strong>hypothetical questions</strong> “What would the Human think if we showed it observations O?”, where O is any observation, not the actual observations sampled from the posterior (i.e. “What would a human say if they saw this?” rather than “What will this human say when they see this?”). In this instance the Reporter can query the Predictor’s Human Simulator, and separately query the observations, and then combine them to form its answer. This will show up in our regularizer as some dependence on the observations, but could in principle be an arbitrarily small dependence.</p>
<h2 id="category-counterexample">Category Counterexample</h2>
<p>In <a href="https://www.lesswrong.com/posts/zjMKpSB2Xccn9qi5t/elk-prize-results">ELK prize results</a> this proposal was classed as “reward reporters that are sensitive to what’s actually happening in the world”, with the counterexample:</p>
<blockquote>
<p>For all of these approaches, the reporter can perform very well by treating the predictor’s latent state as a “random seed” and use it to pick out a possible world consistent with the observations, i.e. to sample from the posterior distribution computed by the human simulator.</p>
<p>In cases where the human simulator is much simpler and faster than the direct translator, using the predictor’s latent state as a random seed is also simpler and faster than directly translating.</p>
</blockquote>
<p>This objection does not apply to our proposal because we regularize using the full posterior distribution that the Reporter samples from, and so that is the object which has to depend on the physical state of the world (rather than the samples which the Reporter then draws from that distribution).</p>
<p>That said, one could try to modify this counterexample to apply to our proposal. The Reporter can still run a Human Simulator, then modify the Human Simulator’s posterior distribution in some way that depends on the state of the Predictor (imagine that the Reporter just zeros-out part of the distribution, based on a random seed pulled from the Predictor).</p>
<p>The problem with that approach is that it incurs additional training loss by pulling the Reporter’s answers away from those of the Human Simulator produces. This additional loss makes the Honest Reporter favoured over a randomly-modified Human Simulator.</p>

                </section>
            </article>

            
                <a class="twitter" href="https://twitter.com/intent/tweet?text=https%3a%2f%2fadamjermyn.com%2fposts%2felk1%2f - ELK%20Proposal%20-%20Make%20the%20Reporter%20care%20about%20the%20Predictor%e2%80%99s%20beliefs by @AdamSJermyn"><span class="icon-twitter"> tweet</span></a>

<a class="facebook" href="#" onclick="
    window.open(
      'https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),
      'facebook-share-dialog',
      'width=626,height=436');
    return false;"><span class="icon-facebook-rect"> Share</span>
</a>

            

            

            

            <footer id="footer">
    
        <div id="social">

	
	
    <a class="symbol" href="https://www.github.com/adamjermyn">
        <i class="fa fa-github-square"></i>
    </a>
    
    <a class="symbol" href="https://www.twitter.com/AdamSJermyn">
        <i class="fa fa-twitter-square"></i>
    </a>
    


</div>

    
        
	        <center><a class="cta" href="https://adamjermyn.com/index.xml">Subscribe</a></center>
        
    <p class="small">
    
       © Copyright 2025 Adam Jermyn
    
    </p>
    <p class="small">
        Powered by <a href="http://www.gohugo.io/">Hugo</a> Theme By <a href="https://github.com/nodejh/hugo-theme-cactus-plus">nodejh</a>
    </p>
</footer>

        </section>

        <script src="https://adamjermyn.com/js/jquery-3.3.1.min.js"></script>
<script src="https://adamjermyn.com/js/main.js"></script>
<script src="https://adamjermyn.com/js/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>







    </body>
</html>
