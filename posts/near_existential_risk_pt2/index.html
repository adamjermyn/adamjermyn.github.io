<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		
		<meta name="author" content="Adam Jermyn">
		<meta name="description" content="Site Description">
		<meta name="generator" content="Hugo 0.82.1" />
		<title>Toy Model of Near-Existential Risk &middot; Adam Jermyn</title>
		<link rel="shortcut icon" href="https://adamjermyn.com/images/favicon.ico">
		<link rel="stylesheet" href="https://adamjermyn.com/css/style.css">
		<link rel="stylesheet" href="https://adamjermyn.com/css/highlight.css">

		
		<link rel="stylesheet" href="https://adamjermyn.com/css/font-awesome.min.css">
		

		
		<link href="https://adamjermyn.com/index.xml" rel="alternate" type="application/rss+xml" title="Adam Jermyn" />
		

		

                
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-186473758-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
		
                <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


	</head>

    <body>
       <nav class="main-nav">
	

        <a href='https://adamjermyn.com/'>About</a>
	<a href='https://adamjermyn.com/posts'>Posts</a>
	<a href='https://adamjermyn.com/tags'>Tags</a>
        <a href='https://adamjermyn.com/software'>Software</a>
        <a href='https://adamjermyn.com/workflow'>Workflow</a>

	

</nav>


        <section id="wrapper" class="post">
            <article>
                <header>
                    <h1>
                        Toy Model of Near-Existential Risk
                    </h1>
                    <h2 class="headline">
		    
                     Apr 23, 2021 16:05 
                    · 1218 words
                    · 6 minute read
                      <span class="tags">
                      
                      
                          
                              <a href="https://adamjermyn.com/tags/longtermism">Longtermism</a>
                          
                      
                      
                      </span>
                    </h2>
                </header>
                
                <section id="post-body">
                    <p>I recently wrote about <a href="https://adamjermyn.com/posts/near_existential_risk/">near-existential risk</a>. These are risks which result in massive losses to society of people/technology/capability, but which themselves do not actually end or permanently curtail human existence. Here I want to build a toy model for thinking about these risks and how they relate to actual existential risks.</p>
<h2 id="a-toy-model">A Toy Model</h2>
<p>For these purposes I&rsquo;m going to reduce the many dimensions of loss to a single parameter $\ell$. When $\ell = 0$ nothing is lost, when $\ell = 1$ all is lost. $\ell=1$ therefore denotes existential risks. Think of $\ell$ as a proxy for &lsquo;what fraction of people/technology/capability/organization/etc. are lost?&rsquo;</p>
<p>Similarly, I&rsquo;m going to reduce the many dimensions of civilizational capability to a parameter $c$. The greater $c$ is the more robust society is against risks, and so an increase in $c$ results in a <em>decrease</em> of risks.</p>
<p>To relate $c$, $\ell$, and the probability of risks, I&rsquo;m going to ignore smaller risks with $\ell &lt; 1/2$  and assume that larger risks with $\ell &gt; 1/2$ are distributed as
$$
\frac{d^2 P(\ell)}{dt d\ell} = \frac{k}{2^c \ell^{c}}
$$
That is, the probability per unit time of a risk of size $\ell$  is proportional to $\ell^{-c}$. Here $k$  is a constant with units of ${\rm time}^{-1}$ and the factor of $2^{c}$ just normalizes the distribution so that overall odds of any risks fall with increasing $c$. For mathematical ease I&rsquo;m going to allow risks with $\ell &gt; 1$: these are all existential.</p>
<p>This captures the notion that risks are power-law distributed (i.e. a risk that is $N$ times worse is some power of $N$ times less likely). Many risks follow power-law distributions, including <a href="https://en.wikipedia.org/wiki/Gutenberg%E2%80%93Richter_law">earthquakes</a> and certain kinds of <a href="http://hornacek.coa.edu/dave/Junk/farmer2004u.pdf">financial risk</a>, and the broader class of fat-failed distribution is part of the premise behind <a href="https://en.wikipedia.org/wiki/Black_swan_theory">black swan theory</a> and Nassim Taleb&rsquo;s broader mantra that risks are dominated the <a href="https://www.fooledbyrandomness.com/FatTails.html">tails of the distribution</a>.</p>
<p>Next, I&rsquo;m going to assume that on average capability $c$ increases exponentially with time, with an $e$-folding time $\tau$, but that when a risk is realized (i.e. the loss $\ell$ is incurred) it sets capability back by a fractional amount $\ell$. So typically
$$
c \propto e^{t/\tau}
$$
but occasionally
$$
c \rightarrow c(1-\ell)
$$
with the caveat that when $\ell &gt; 1$, $c \rightarrow 0$.</p>
<p>Before exploring the model in detail, I just want to recap the assumptions:</p>
<ul>
<li>Risks are power-law distributed in how much loss they cause.</li>
<li>Risks that would cause more than 100% loss are capped at 100%.</li>
<li>Capabilities to avert risks grow exponentially with time.</li>
</ul>
<p>The most suspect of these to my eye is the last one: civilization currently is robust against certain risks that were much worse in the pre-industrial era  (e.g. sudden drought/crop failure) but is much more fragile against brand new kinds of risk, like nuclear war.</p>
<h2 id="model-predictions">Model Predictions</h2>
<p>Setting that aside, what does this model predict?</p>
<p>Over time risks fall on average. The odds of an existential risk are proportional to
$$
\frac{dP(\ell &gt; 1)}{dt} = \int_{1}^{\infty} \frac{k}{2^c\ell^{c}} d\ell = \frac{k}{2^c(c-1)}
$$
which decreases as civilizational capability $c$ increases. Because of this, almost all of the risk happens at early times. At late times $c$ is enormous and we&rsquo;re in the clear. In particular, if the starting capability $c_0$ is large and if we ignore all non-existential risks then the total odds of existential risk are
$$
P_{\rm ex} \approx \int_0^\infty \frac{k}{2^{c_0 e^{t/\tau}} (c_0 e^{t/\tau}-1)} dt \approx \frac{k\tau}{2^{c_0} c_0}
$$
Here I&rsquo;ve very crudely approximated the integral by noting that the denominator becomes large on a timescale of $\tau$, so the integral is roughly the integrand at $t=0$ times $\tau$.</p>
<p>At the same time, in this model near-existential risks look pretty bad. Again assuming $c_0 \gg 1$, the odds of existential risk per unit time are roughly $k/c_0$. If suddenly a risk of $\ell=0.9$ happens then the odds of existential risk rise from $k/c_0$ to $10k/c_0$, a ten-fold increase.</p>
<p>More generally, how much larger is $P_{\rm ex}$ once we incorporate near-existential risks? This is like asking about the odds of the pathway of (near-existential risk $\rightarrow$ existential risk).</p>
<p>For these purposes I&rsquo;ll call risks that have $\ell &gt; 1/2$ near-existential (imagine half of all cities disappearing overnight). The odds per unit time of an event with $1/2 &lt; \ell &lt; 1$ are
$$
\frac{dP(1/2 &lt; \ell &lt; 1)}{dt} = \int_{1/2}^{1} \frac{k}{\ell^{c}} d\ell = k\frac{2^{c-1}-1}{2^c (c-1)} \approx \frac{k}{2 c}
$$
which is a factor of $2^{c-1}$ times more likely than existential risks. If one of these risks happen though, the odds of an existential risk increase by a factor of roughly $1/(1-\ell)$, and remain elevated by that much for a time of order $\tau$ (the civilization rebuilding time). Even if all near-existential risks had $\ell=1/2$, that doubles the odds of existential risks for a time $\tau$ before they fall back down. Specifically, the change in $P_{\rm ex}$ is of order
$$
\Delta P_{\rm ex} \approx \int_0^{\infty} \frac{dP(1/2&lt;\ell&lt;1)}{dt'}\int_{t'}^{\infty} \frac{dP(\ell &gt; 1)}{dt} dt  dt' \approx \int_0^{\infty}\frac{k^2\tau}{2^{c_0 e^{t'/\tau}} c_0^2 e^{2 t'/\tau}} dt' \approx \frac{(k\tau)^2}{2^{c_0} c_0^2}
$$
where the double integral is summing over all possible paths of the form &ldquo;time $t'$ passes before the near-existential risk, then some amount of time later an existential risk happens&rdquo;. This isn&rsquo;t quite right because it ignores the odds of an existential risk happening first, or of multiple non-existential risks, but if all probabilities involved are small the error shouldn&rsquo;t be too large.</p>
<p>The expression above is unwieldy, but can be written instead as
$$
\Delta P_{\rm ex} \approx \frac{k \tau}{c_0} P_{\rm ex}
$$
In other words, whether near-existential risks matter for purposes of existential risk depends entierly on the ratio $k\tau/c_0$. What is that ratio? Unfortunately this is very hard to estimate because (fortunately) large risks of this sort are very rare.</p>
<h2 id="rough-estimates">Rough Estimates</h2>
<p>As an extremely rough base rate anchor,  $c_0/k$ is the typical timescale on which we expect large risks ($\ell \sim 1/2$) to occur, and $\tau$ is the timescale on which civilization rebuilds from large losses.  The last $\ell \sim 1/2$ global disaster to occur was the <a href="https://en.wikipedia.org/wiki/Black_Death">Plague</a> in the 1300s, which killed an estimated 1/4 of all people (30-60% of people in Europe), which suggests that $c_0 / k$ is of order 700 years. World population took a century or so to recover (based on some very quick googling), suggesting that $k\tau/c_0 \approx 1/7$. If that&rsquo;s the case, near-existential risks do not contribute all that much to the overall odds of existential risk.</p>
<p>An alternate anchor is to reason with the inside view. The inside view here says that at least some future near-existential risks have odds of order 0.1 percent per-year[<a href="https://www.metaculus.com/questions/3517/will-there-be-a-global-thermonuclear-war-by-2070/">1</a>,<a href="https://www.metaculus.com/questions/1585/ragnar%25C3%25B6k-question-series-if-a-nuclear-catastrophe-occurs-will-it-reduce-the-human-population-by-95-or-more/">2</a>,<a href="https://www.metaculus.com/questions/1493/ragnar%25C3%25B6k-question-series-by-2100-will-the-human-population-decrease-by-at-least-10-during-any-period-of-5-years/">3</a>] which means $c_0 / k$ is of order 1000 years. Rebuilding times seem likely to be of order the time it takes for countries to industrialize, which is 50 or so years, giving $k\tau / c_0 \approx 1/20$. The exact figure depends on what size of disaster &ldquo;counts&rdquo; for these purposes and what counts as a recovery, but this serves to set an order of magnitude and agrees surprisingly well with the base rate answer.</p>
<p>These estimates suggest that near-existential risks are not a likely pathway to bad-existential outcomes relative to direct existential risks, in contrast to my intuition from <a href="https://adamjermyn.com/posts/near_existential_risk/">before</a>, and this model has caused me to update in the direction of worrying more about existential than near-existential risks.</p>

                </section>
            </article>

            
                <a class="twitter" href="https://twitter.com/intent/tweet?text=https%3a%2f%2fadamjermyn.com%2fposts%2fnear_existential_risk_pt2%2f - Toy%20Model%20of%20Near-Existential%20Risk by @AdamSJermyn"><span class="icon-twitter"> tweet</span></a>

<a class="facebook" href="#" onclick="
    window.open(
      'https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),
      'facebook-share-dialog',
      'width=626,height=436');
    return false;"><span class="icon-facebook-rect"> Share</span>
</a>

            

            

            

            <footer id="footer">
    
        <div id="social">

	
	
    <a class="symbol" href="https://www.github.com/adamjermyn">
        <i class="fa fa-github-square"></i>
    </a>
    
    <a class="symbol" href="https://www.twitter.com/AdamSJermyn">
        <i class="fa fa-twitter-square"></i>
    </a>
    


</div>

    
        
	        <center><a class="cta" href="https://adamjermyn.com/index.xml">Subscribe</a></center>
        
    <p class="small">
    
       © Copyright 2021 Adam Jermyn
    
    </p>
    <p class="small">
        Powered by <a href="http://www.gohugo.io/">Hugo</a> Theme By <a href="https://github.com/nodejh/hugo-theme-cactus-plus">nodejh</a>
    </p>
</footer>

        </section>

        <script src="https://adamjermyn.com/js/jquery-3.3.1.min.js"></script>
<script src="https://adamjermyn.com/js/main.js"></script>
<script src="https://adamjermyn.com/js/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>







    </body>
</html>
