<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Safety on Adam Jermyn</title>
    <link>https://adamjermyn.com/tags/ai-safety/</link>
    <description>Recent content in AI Safety on Adam Jermyn</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 27 Jun 2022 10:15:31 -0400</lastBuildDate><atom:link href="https://adamjermyn.com/tags/ai-safety/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Training Trace Priors and Speed Priors</title>
      <link>https://adamjermyn.com/posts/tracespeed/</link>
      <pubDate>Mon, 27 Jun 2022 10:15:31 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/tracespeed/</guid>
      <description>Posted to the AI Alignment Forum here.
Thanks to Evan Hubinger for suggesting this idea.
Training Trace Priors are priors over boolean circuits which examine the outputs of gates on samples from the training distribution, typically for purposes of steering models away from having components that were never tested during training. The one I like to think about is the One-Gate Trace Prior (OGT Prior), which penalizes circuits if there are gates with constant outputs during training.</description>
    </item>
    
    <item>
      <title>Conditioning Generative Models</title>
      <link>https://adamjermyn.com/posts/conditioninggenerativemodels/</link>
      <pubDate>Mon, 27 Jun 2022 10:14:16 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/conditioninggenerativemodels/</guid>
      <description>Posted to the AI Alignment Forum here.
This post was written in response to Evan Hubinger’s shortform prompt below, and benefited from discussions with him.
 Suppose you had a language model that you knew was in fact a good generative model of the world and that this property continued to hold regardless of what you conditioned it on. Furthermore, suppose you had some prompt that described some agent for the language model to simulate (Alice) that in practice resulted in aligned-looking outputs.</description>
    </item>
    
    <item>
      <title>Multigate Priors</title>
      <link>https://adamjermyn.com/posts/multigate_prior/</link>
      <pubDate>Wed, 15 Jun 2022 15:33:51 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/multigate_prior/</guid>
      <description>Posted to LessWrong here.
(Thanks to Evan Hubinger and Nicholas Schiefer for suggestions and discussions around these ideas)
Multi-Gate Traces We can improve on one-gate traces with multi-gate traces! Suppose we have $N$ training samples. Then we can reasonably estimate joint probability distributions over up to $k=\lfloor\log_2 N\rfloor$ gates, giving the prior:
$$ p\propto \exp\left(-\sum_{i_1&amp;hellip;i_k}\sum_{j\in [0,1]^k}p_{i_1&amp;hellip;i_k}(j)^m\right) $$
That is, we sum over all selections of $k$ gates and compute the entropy of their joint distribution over $2^k$ states.</description>
    </item>
    
    <item>
      <title>Trace Training Priors</title>
      <link>https://adamjermyn.com/posts/trace_training/</link>
      <pubDate>Mon, 13 Jun 2022 13:14:01 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/trace_training/</guid>
      <description>(Crossposted from the AI Alignment Forums)
I’m worried about scenarios involving deceptive models. We’ve failed at inner alignment so the model has goals that are not aligned with ours. It can somehow detect when it’s in training, and during training it pretends to share our goals. During deployment, surprise! The model paperclips the universe.
In this story deception is all about the model having hidden behaviors that never get triggered during training.</description>
    </item>
    
    <item>
      <title>ELK Proposal - Make the Reporter care about the Predictor’s beliefs</title>
      <link>https://adamjermyn.com/posts/elk1/</link>
      <pubDate>Mon, 13 Jun 2022 09:27:54 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/elk1/</guid>
      <description>Posted to the AI Alignment Forum here.
(This proposal received an honorable mention in the ELK prize results, and we believe was classified among strategies which “reward reporters that are sensitive to what’s actually happening in the world”. We do not think that the counterexample to that class of strategies works against our proposal, though, and we have explained why in a note at the end. Feedback, disagreement, and new failure modes are very welcome!</description>
    </item>
    
    <item>
      <title>Give the model a model-builder</title>
      <link>https://adamjermyn.com/posts/give_model_builder/</link>
      <pubDate>Mon, 06 Jun 2022 16:54:20 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/give_model_builder/</guid>
      <description>(Thanks to Evan Hubinger for suggesting this problem and for comments on these ideas. Feedback is very welcome. Cross-posted from LessWrong.)
The Setup Suppose we have a prior $p_{\rm good}(\mathrm{model})$ that we think is non-deceptive, such that we can sample from this prior and get good models that are inner-aligned.
These models may, for instrumental purposes, need to themselves produce models (via searches/optimization/meta-learning). For instance we might ask an outer model to play poker.</description>
    </item>
    
    <item>
      <title>Give the AI safe tools</title>
      <link>https://adamjermyn.com/posts/give_ai_tools/</link>
      <pubDate>Sat, 04 Jun 2022 08:21:00 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/give_ai_tools/</guid>
      <description>(Cross-posted from LessWrong)
One kind of concern with AI is that:
 There are some tools that are instrumentally useful for an AI to have. Most/the most accessible versions of those tools are dangerous. The AI doesn’t care which versions are dangerous. Hence, the AI will probably develop dangerous tools for instrumental reasons.  You might call concerns like this Instrumental Danger Problems. This post aims to examine some existing approaches to Instrumental Danger Problems, and to introduce a new one, namely “Giving the AI safe tools”.</description>
    </item>
    
    <item>
      <title>AI risk in a few words</title>
      <link>https://adamjermyn.com/posts/ai_risk_few_words/</link>
      <pubDate>Mon, 14 Mar 2022 14:27:25 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/ai_risk_few_words/</guid>
      <description>AI safety is a weird area, and in trying to explain it to friends I realized that I don&amp;rsquo;t have a simple explanation of what the (existential) risks are and why I think they&amp;rsquo;re important.
Word limits force simplicity, so here are some attempts to explain AI risk in different numbers of words.
150 words  Do you value the same things as your great grandfather? What we value changes from generation to generation.</description>
    </item>
    
  </channel>
</rss>
