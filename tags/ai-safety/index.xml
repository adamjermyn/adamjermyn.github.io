<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Safety on Adam Jermyn</title>
    <link>https://adamjermyn.com/tags/ai-safety/</link>
    <description>Recent content in AI Safety on Adam Jermyn</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 17 Aug 2022 16:55:03 -0400</lastBuildDate><atom:link href="https://adamjermyn.com/tags/ai-safety/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Conditioning, Prompts, and Fine-Tuning</title>
      <link>https://adamjermyn.com/posts/conditioning_fine_tuning/</link>
      <pubDate>Wed, 17 Aug 2022 16:55:03 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/conditioning_fine_tuning/</guid>
      <description>Posted to the AI Alignment Forums.
(Thanks to Evan Hubinger and Nicholas Schiefer for comments on these ideas.)
These are some notes on the relation between conditioning language models, prompting, and fine-tuning. The key takeaways are:
 Prompting and fine-tuning can both be used to condition language models. Prompting is quite restricted in the kinds of conditionals it can achieve. Fine-tuning can implement arbitrary conditionals in principle, though not in practice.</description>
    </item>
    
    <item>
      <title>Conditioning Generative Models with Restrictions</title>
      <link>https://adamjermyn.com/posts/conditioninggenerativemodels2/</link>
      <pubDate>Thu, 21 Jul 2022 16:34:05 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/conditioninggenerativemodels2/</guid>
      <description>This is a followup to Conditioning Generative Models based on further discussions with Evan Hubinger, Nicholas Schiefer, Abram Demski, Curtis Huebner, Hoagy Cunningham, Derek Shiller, and James Lucassen, as well as broader conversations with many different people at the recent ARC/ELK retreat. For more background on this general direction see Johannes Treutlein’s “Training goals for large language models”.
Background Previously, I wrote about ways we could use a generative language model to produce alignment research.</description>
    </item>
    
    <item>
      <title>Quantilizers and Generative Models</title>
      <link>https://adamjermyn.com/posts/quantilizers/</link>
      <pubDate>Mon, 18 Jul 2022 12:33:31 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/quantilizers/</guid>
      <description>(Cross-posted from the AI Alignment Forum)
Thanks to Evan Hubinger for discussions about quantilizers, and to James Lucassen for discussions about conditioned generative models. Many of these ideas are discussed in Quantilizers: A Safer Alternative to Maximizers for Limited Optimization: this post just expands on a particular thread of ideas in that paper. Throughout I’ll refer to sections of the paper. I have some remaining confusion about the “targeted impact” section, and would appreciate clarifications/corrections!</description>
    </item>
    
    <item>
      <title>Grouped Loss may disfavor discontinuous capabilities</title>
      <link>https://adamjermyn.com/posts/grouped_loss/</link>
      <pubDate>Sat, 09 Jul 2022 08:21:00 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/grouped_loss/</guid>
      <description>(Cross-posted from the AI Alignment Forum)
Thanks to Evan Hubinger and Beth Barnes for comments on these ideas.
Language models exhibit clear scaling laws, where the loss is a power-law in model size. This offers a lot of predictive power, and seems like a useful thing to know. By contrast, individual capabilities often exhibit sharp discontinuities in performance as a function of model size and training time.
It would be great if individual capabilities just gradually improved like the broader loss.</description>
    </item>
    
    <item>
      <title>Latent Adversarial Training</title>
      <link>https://adamjermyn.com/posts/lat/</link>
      <pubDate>Wed, 29 Jun 2022 16:05:25 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/lat/</guid>
      <description>Posted to the AI Alignment Forum.
The Problem We’d like to train models to be robustly safe, even in environments that may fall well outside of the training distribution. Unfortunately all we get to work with is the training distribution, which makes ensuring robust generalization difficult.
Deception is an example of this concern. Models may be well-behaved in the training environment because they realize it is a training environment, but behave poorly in deployment because they notice the distributional shift.</description>
    </item>
    
    <item>
      <title>Training Trace Priors and Speed Priors</title>
      <link>https://adamjermyn.com/posts/tracespeed/</link>
      <pubDate>Mon, 27 Jun 2022 10:15:31 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/tracespeed/</guid>
      <description>Posted to the AI Alignment Forum here.
Thanks to Evan Hubinger for suggesting this idea.
Training Trace Priors are priors over boolean circuits which examine the outputs of gates on samples from the training distribution, typically for purposes of steering models away from having components that were never tested during training. The one I like to think about is the One-Gate Trace Prior (OGT Prior), which penalizes circuits if there are gates with constant outputs during training.</description>
    </item>
    
    <item>
      <title>Conditioning Generative Models</title>
      <link>https://adamjermyn.com/posts/conditioninggenerativemodels/</link>
      <pubDate>Mon, 27 Jun 2022 10:14:16 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/conditioninggenerativemodels/</guid>
      <description>Posted to the AI Alignment Forum here.
This post was written in response to Evan Hubinger’s shortform prompt below, and benefited from discussions with him.
 Suppose you had a language model that you knew was in fact a good generative model of the world and that this property continued to hold regardless of what you conditioned it on. Furthermore, suppose you had some prompt that described some agent for the language model to simulate (Alice) that in practice resulted in aligned-looking outputs.</description>
    </item>
    
    <item>
      <title>Multigate Priors</title>
      <link>https://adamjermyn.com/posts/multigate_prior/</link>
      <pubDate>Wed, 15 Jun 2022 15:33:51 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/multigate_prior/</guid>
      <description>Posted to LessWrong here.
(Thanks to Evan Hubinger and Nicholas Schiefer for suggestions and discussions around these ideas)
Multi-Gate Traces We can improve on one-gate traces with multi-gate traces! Suppose we have $N$ training samples. Then we can reasonably estimate joint probability distributions over up to $k=\lfloor\log_2 N\rfloor$ gates, giving the prior:
$$ p\propto \exp\left(-\sum_{i_1&amp;hellip;i_k}\sum_{j\in [0,1]^k}p_{i_1&amp;hellip;i_k}(j)^m\right) $$
That is, we sum over all selections of $k$ gates and compute the entropy of their joint distribution over $2^k$ states.</description>
    </item>
    
    <item>
      <title>Trace Training Priors</title>
      <link>https://adamjermyn.com/posts/trace_training/</link>
      <pubDate>Mon, 13 Jun 2022 13:14:01 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/trace_training/</guid>
      <description>(Crossposted from the AI Alignment Forums)
I’m worried about scenarios involving deceptive models. We’ve failed at inner alignment so the model has goals that are not aligned with ours. It can somehow detect when it’s in training, and during training it pretends to share our goals. During deployment, surprise! The model paperclips the universe.
In this story deception is all about the model having hidden behaviors that never get triggered during training.</description>
    </item>
    
    <item>
      <title>ELK Proposal - Make the Reporter care about the Predictor’s beliefs</title>
      <link>https://adamjermyn.com/posts/elk1/</link>
      <pubDate>Mon, 13 Jun 2022 09:27:54 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/elk1/</guid>
      <description>Posted to the AI Alignment Forum here.
(This proposal received an honorable mention in the ELK prize results, and we believe was classified among strategies which “reward reporters that are sensitive to what’s actually happening in the world”. We do not think that the counterexample to that class of strategies works against our proposal, though, and we have explained why in a note at the end. Feedback, disagreement, and new failure modes are very welcome!</description>
    </item>
    
    <item>
      <title>Give the model a model-builder</title>
      <link>https://adamjermyn.com/posts/give_model_builder/</link>
      <pubDate>Mon, 06 Jun 2022 16:54:20 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/give_model_builder/</guid>
      <description>(Thanks to Evan Hubinger for suggesting this problem and for comments on these ideas. Feedback is very welcome. Cross-posted from LessWrong.)
The Setup Suppose we have a prior $p_{\rm good}(\mathrm{model})$ that we think is non-deceptive, such that we can sample from this prior and get good models that are inner-aligned.
These models may, for instrumental purposes, need to themselves produce models (via searches/optimization/meta-learning). For instance we might ask an outer model to play poker.</description>
    </item>
    
    <item>
      <title>Brief Notes on Transformers</title>
      <link>https://adamjermyn.com/posts/transformer_notes/</link>
      <pubDate>Sat, 04 Jun 2022 08:21:00 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/transformer_notes/</guid>
      <description>(Cross-posted from the AI Alignment Forum)*
*These are just some notes I wrote while reading about transformers, which I thought might be a useful reference to others. Corrections welcome.
Overview of Transformers Many transformer models have the following architecture:
Data flows as follows:
 We take tokens as inputs and pass them through an embedding layer. The embedding layer outputs its result into the residual stream (x0). This has dimension (C,E), where C is the number of tokens in the context window and E is the embedding dimension.</description>
    </item>
    
    <item>
      <title>Give the AI safe tools</title>
      <link>https://adamjermyn.com/posts/give_ai_tools/</link>
      <pubDate>Sat, 04 Jun 2022 08:21:00 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/give_ai_tools/</guid>
      <description>(Cross-posted from LessWrong)
One kind of concern with AI is that:
 There are some tools that are instrumentally useful for an AI to have. Most/the most accessible versions of those tools are dangerous. The AI doesn’t care which versions are dangerous. Hence, the AI will probably develop dangerous tools for instrumental reasons.  You might call concerns like this Instrumental Danger Problems. This post aims to examine some existing approaches to Instrumental Danger Problems, and to introduce a new one, namely “Giving the AI safe tools”.</description>
    </item>
    
    <item>
      <title>AI risk in a few words</title>
      <link>https://adamjermyn.com/posts/ai_risk_few_words/</link>
      <pubDate>Mon, 14 Mar 2022 14:27:25 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/ai_risk_few_words/</guid>
      <description>AI safety is a weird area, and in trying to explain it to friends I realized that I don&amp;rsquo;t have a simple explanation of what the (existential) risks are and why I think they&amp;rsquo;re important.
Word limits force simplicity, so here are some attempts to explain AI risk in different numbers of words.
150 words  Do you value the same things as your great grandfather? What we value changes from generation to generation.</description>
    </item>
    
  </channel>
</rss>
