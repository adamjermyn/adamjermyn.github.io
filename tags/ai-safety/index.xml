<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Safety on Adam Jermyn</title>
    <link>https://adamjermyn.com/tags/ai-safety/</link>
    <description>Recent content in AI Safety on Adam Jermyn</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 13 Jun 2022 09:27:54 -0400</lastBuildDate><atom:link href="https://adamjermyn.com/tags/ai-safety/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ELK Proposal - Make the Reporter care about the Predictor’s beliefs</title>
      <link>https://adamjermyn.com/posts/elk1/</link>
      <pubDate>Mon, 13 Jun 2022 09:27:54 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/elk1/</guid>
      <description>Posted to the AI Alignment Forum here.
(This proposal received an honorable mention in the ELK prize results, and we believe was classified among strategies which “reward reporters that are sensitive to what’s actually happening in the world”. We do not think that the counterexample to that class of strategies works against our proposal, though, and we have explained why in a note at the end. Feedback, disagreement, and new failure modes are very welcome!</description>
    </item>
    
    <item>
      <title>Give the model a model-builder</title>
      <link>https://adamjermyn.com/posts/give_model_builder/</link>
      <pubDate>Mon, 06 Jun 2022 16:54:20 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/give_model_builder/</guid>
      <description>(Thanks to Evan Hubinger for suggesting this problem and for comments on these ideas. Feedback is very welcome. Cross-posted from LessWrong.)
The Setup Suppose we have a prior $p_{\rm good}(\mathrm{model})$ that we think is non-deceptive, such that we can sample from this prior and get good models that are inner-aligned.
These models may, for instrumental purposes, need to themselves produce models (via searches/optimization/meta-learning). For instance we might ask an outer model to play poker.</description>
    </item>
    
    <item>
      <title>Give the AI safe tools</title>
      <link>https://adamjermyn.com/posts/give_ai_tools/</link>
      <pubDate>Sat, 04 Jun 2022 08:21:00 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/give_ai_tools/</guid>
      <description>(Cross-posted from LessWrong)
One kind of concern with AI is that:
 There are some tools that are instrumentally useful for an AI to have. Most/the most accessible versions of those tools are dangerous. The AI doesn’t care which versions are dangerous. Hence, the AI will probably develop dangerous tools for instrumental reasons.  You might call concerns like this Instrumental Danger Problems. This post aims to examine some existing approaches to Instrumental Danger Problems, and to introduce a new one, namely “Giving the AI safe tools”.</description>
    </item>
    
    <item>
      <title>AI risk in a few words</title>
      <link>https://adamjermyn.com/posts/ai_risk_few_words/</link>
      <pubDate>Mon, 14 Mar 2022 14:27:25 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/ai_risk_few_words/</guid>
      <description>AI safety is a weird area, and in trying to explain it to friends I realized that I don&amp;rsquo;t have a simple explanation of what the (existential) risks are and why I think they&amp;rsquo;re important.
Word limits force simplicity, so here are some attempts to explain AI risk in different numbers of words.
150 words  Do you value the same things as your great grandfather? What we value changes from generation to generation.</description>
    </item>
    
  </channel>
</rss>
