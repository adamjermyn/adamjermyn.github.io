<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on Adam Jermyn</title>
    <link>https://adamjermyn.com/tags/ai/</link>
    <description>Recent content in AI on Adam Jermyn</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 04 Jan 2026 12:00:00 -0500</lastBuildDate><atom:link href="https://adamjermyn.com/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Tragedy of Almost: Arline Feynman and the Information Gap</title>
      <link>https://adamjermyn.com/posts/arline_feynman/</link>
      <pubDate>Sun, 04 Jan 2026 12:00:00 -0500</pubDate>
      
      <guid>https://adamjermyn.com/posts/arline_feynman/</guid>
      <description>How many people have died not because treatments didn&amp;rsquo;t exist, but because the information didn&amp;rsquo;t reach them in time? Every new cure has a window where it exists but isn&amp;rsquo;t widely known, isn&amp;rsquo;t easily accessible, requires connections or luck to obtain. Someone is always on the wrong side of that window.
One of the cruelest examples: Arline Feynman, wife of physicist Richard Feynman, who died of tuberculosis in June 1945.</description>
    </item>
    
    <item>
      <title>Lectures on Fractures</title>
      <link>https://adamjermyn.com/posts/lectures_on_fractures/</link>
      <pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate>
      
      <guid>https://adamjermyn.com/posts/lectures_on_fractures/</guid>
      <description>I recently collaborated with Claude Opus 4.5 to write a short set of lecture notes on fracture mechanics:
Lectures on Fractures (PDF)
How it worked I wanted something in Feynman&amp;rsquo;s style. Conversational, building intuition before formalism, full of physical insight. So I had Claude start by reading some of Feynman&amp;rsquo;s writing and assembling a style guide. That gave us a shared reference for tone.
The technical focus was Griffith&amp;rsquo;s work on fractures.</description>
    </item>
    
    <item>
      <title>Querying ultra-long contexts with summary trees</title>
      <link>https://adamjermyn.com/posts/claude_summary/</link>
      <pubDate>Sun, 09 Jun 2024 18:24:21 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/claude_summary/</guid>
      <description>Views my own. There might well be more clever ways to do all of this that I don&amp;rsquo;t know of.
Suppose you have some long, ordered text data, like news stories going back a century or journal entries or server logs. You&amp;rsquo;d like to analyze them with a Large Language Model (LLM), but you can&amp;rsquo;t just concatenate them together (the context window isn&amp;rsquo;t that long!).
A fun solution to this problem is a summary tree.</description>
    </item>
    
    <item>
      <title>Brief Notes on Transformers</title>
      <link>https://adamjermyn.com/posts/transformer_notes/</link>
      <pubDate>Mon, 26 Sep 2022 08:21:00 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/transformer_notes/</guid>
      <description>(Cross-posted from the AI Alignment Forum)*
*These are just some notes I wrote while reading about transformers, which I thought might be a useful reference to others. Corrections welcome.
Overview of Transformers Many transformer models have the following architecture:
Data flows as follows:
 We take tokens as inputs and pass them through an embedding layer. The embedding layer outputs its result into the residual stream (x0). This has dimension (C,E), where C is the number of tokens in the context window and E is the embedding dimension.</description>
    </item>
    
    <item>
      <title>Conditioning, Prompts, and Fine-Tuning</title>
      <link>https://adamjermyn.com/posts/conditioning_fine_tuning/</link>
      <pubDate>Wed, 17 Aug 2022 16:55:03 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/conditioning_fine_tuning/</guid>
      <description>Posted to the AI Alignment Forums.
(Thanks to Evan Hubinger and Nicholas Schiefer for comments on these ideas.)
These are some notes on the relation between conditioning language models, prompting, and fine-tuning. The key takeaways are:
 Prompting and fine-tuning can both be used to condition language models. Prompting is quite restricted in the kinds of conditionals it can achieve. Fine-tuning can implement arbitrary conditionals in principle, though not in practice.</description>
    </item>
    
    <item>
      <title>Conditioning Generative Models with Restrictions</title>
      <link>https://adamjermyn.com/posts/conditioninggenerativemodels2/</link>
      <pubDate>Thu, 21 Jul 2022 16:34:05 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/conditioninggenerativemodels2/</guid>
      <description>This is a followup to Conditioning Generative Models based on further discussions with Evan Hubinger, Nicholas Schiefer, Abram Demski, Curtis Huebner, Hoagy Cunningham, Derek Shiller, and James Lucassen, as well as broader conversations with many different people at the recent ARC/ELK retreat. For more background on this general direction see Johannes Treutlein’s “Training goals for large language models”.
Background Previously, I wrote about ways we could use a generative language model to produce alignment research.</description>
    </item>
    
    <item>
      <title>Quantilizers and Generative Models</title>
      <link>https://adamjermyn.com/posts/quantilizers/</link>
      <pubDate>Mon, 18 Jul 2022 12:33:31 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/quantilizers/</guid>
      <description>(Cross-posted from the AI Alignment Forum)
Thanks to Evan Hubinger for discussions about quantilizers, and to James Lucassen for discussions about conditioned generative models. Many of these ideas are discussed in Quantilizers: A Safer Alternative to Maximizers for Limited Optimization: this post just expands on a particular thread of ideas in that paper. Throughout I’ll refer to sections of the paper. I have some remaining confusion about the “targeted impact” section, and would appreciate clarifications/corrections!</description>
    </item>
    
    <item>
      <title>Grouped Loss may disfavor discontinuous capabilities</title>
      <link>https://adamjermyn.com/posts/grouped_loss/</link>
      <pubDate>Sat, 09 Jul 2022 08:21:00 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/grouped_loss/</guid>
      <description>(Cross-posted from the AI Alignment Forum)
Thanks to Evan Hubinger and Beth Barnes for comments on these ideas.
Language models exhibit clear scaling laws, where the loss is a power-law in model size. This offers a lot of predictive power, and seems like a useful thing to know. By contrast, individual capabilities often exhibit sharp discontinuities in performance as a function of model size and training time.
It would be great if individual capabilities just gradually improved like the broader loss.</description>
    </item>
    
    <item>
      <title>Latent Adversarial Training</title>
      <link>https://adamjermyn.com/posts/lat/</link>
      <pubDate>Wed, 29 Jun 2022 16:05:25 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/lat/</guid>
      <description>Posted to the AI Alignment Forum.
The Problem We’d like to train models to be robustly safe, even in environments that may fall well outside of the training distribution. Unfortunately all we get to work with is the training distribution, which makes ensuring robust generalization difficult.
Deception is an example of this concern. Models may be well-behaved in the training environment because they realize it is a training environment, but behave poorly in deployment because they notice the distributional shift.</description>
    </item>
    
    <item>
      <title>Training Trace Priors and Speed Priors</title>
      <link>https://adamjermyn.com/posts/tracespeed/</link>
      <pubDate>Mon, 27 Jun 2022 10:15:31 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/tracespeed/</guid>
      <description>Posted to the AI Alignment Forum here.
Thanks to Evan Hubinger for suggesting this idea.
Training Trace Priors are priors over boolean circuits which examine the outputs of gates on samples from the training distribution, typically for purposes of steering models away from having components that were never tested during training. The one I like to think about is the One-Gate Trace Prior (OGT Prior), which penalizes circuits if there are gates with constant outputs during training.</description>
    </item>
    
    <item>
      <title>Conditioning Generative Models</title>
      <link>https://adamjermyn.com/posts/conditioninggenerativemodels/</link>
      <pubDate>Mon, 27 Jun 2022 10:14:16 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/conditioninggenerativemodels/</guid>
      <description>Posted to the AI Alignment Forum here.
This post was written in response to Evan Hubinger’s shortform prompt below, and benefited from discussions with him.
 Suppose you had a language model that you knew was in fact a good generative model of the world and that this property continued to hold regardless of what you conditioned it on. Furthermore, suppose you had some prompt that described some agent for the language model to simulate (Alice) that in practice resulted in aligned-looking outputs.</description>
    </item>
    
    <item>
      <title>#SAT with Tensor Networks</title>
      <link>https://adamjermyn.com/posts/tn_bc/</link>
      <pubDate>Fri, 17 Jun 2022 09:21:45 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/tn_bc/</guid>
      <description>Cross-posted from LessWrong.
(Thanks to Mark Xu for comments on these notes.)
These are some notes on tensor networks, the mapping from boolean circuits to tensor networks, and how #SAT can be written as a tensor network contraction.
Definitions and Notation What is a tensor? A rank-$d$ tensor is an array of numbers with $d$ integer indices, e.g. $T_{ijk}$ is a rank-3 tensor. The range of each index is called its bond dimension.</description>
    </item>
    
    <item>
      <title>Multigate Priors</title>
      <link>https://adamjermyn.com/posts/multigate_prior/</link>
      <pubDate>Wed, 15 Jun 2022 15:33:51 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/multigate_prior/</guid>
      <description>Posted to LessWrong here.
(Thanks to Evan Hubinger and Nicholas Schiefer for suggestions and discussions around these ideas)
Multi-Gate Traces We can improve on one-gate traces with multi-gate traces! Suppose we have $N$ training samples. Then we can reasonably estimate joint probability distributions over up to $k=\lfloor\log_2 N\rfloor$ gates, giving the prior:
$$ p\propto \exp\left(-\sum_{i_1&amp;hellip;i_k}\sum_{j\in [0,1]^k}p_{i_1&amp;hellip;i_k}(j)^m\right) $$
That is, we sum over all selections of $k$ gates and compute the entropy of their joint distribution over $2^k$ states.</description>
    </item>
    
    <item>
      <title>Trace Training Priors</title>
      <link>https://adamjermyn.com/posts/trace_training/</link>
      <pubDate>Mon, 13 Jun 2022 13:14:01 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/trace_training/</guid>
      <description>(Crossposted from the AI Alignment Forums)
I’m worried about scenarios involving deceptive models. We’ve failed at inner alignment so the model has goals that are not aligned with ours. It can somehow detect when it’s in training, and during training it pretends to share our goals. During deployment, surprise! The model paperclips the universe.
In this story deception is all about the model having hidden behaviors that never get triggered during training.</description>
    </item>
    
    <item>
      <title>ELK Proposal - Make the Reporter care about the Predictor’s beliefs</title>
      <link>https://adamjermyn.com/posts/elk1/</link>
      <pubDate>Mon, 13 Jun 2022 09:27:54 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/elk1/</guid>
      <description>Posted to the AI Alignment Forum here.
(This proposal received an honorable mention in the ELK prize results, and we believe was classified among strategies which “reward reporters that are sensitive to what’s actually happening in the world”. We do not think that the counterexample to that class of strategies works against our proposal, though, and we have explained why in a note at the end. Feedback, disagreement, and new failure modes are very welcome!</description>
    </item>
    
    <item>
      <title>Give the model a model-builder</title>
      <link>https://adamjermyn.com/posts/give_model_builder/</link>
      <pubDate>Mon, 06 Jun 2022 16:54:20 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/give_model_builder/</guid>
      <description>(Thanks to Evan Hubinger for suggesting this problem and for comments on these ideas. Feedback is very welcome. Cross-posted from LessWrong.)
The Setup Suppose we have a prior $p_{\rm good}(\mathrm{model})$ that we think is non-deceptive, such that we can sample from this prior and get good models that are inner-aligned.
These models may, for instrumental purposes, need to themselves produce models (via searches/optimization/meta-learning). For instance we might ask an outer model to play poker.</description>
    </item>
    
    <item>
      <title>Give the AI safe tools</title>
      <link>https://adamjermyn.com/posts/give_ai_tools/</link>
      <pubDate>Sat, 04 Jun 2022 08:21:00 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/give_ai_tools/</guid>
      <description>(Cross-posted from LessWrong)
One kind of concern with AI is that:
 There are some tools that are instrumentally useful for an AI to have. Most/the most accessible versions of those tools are dangerous. The AI doesn’t care which versions are dangerous. Hence, the AI will probably develop dangerous tools for instrumental reasons.  You might call concerns like this Instrumental Danger Problems. This post aims to examine some existing approaches to Instrumental Danger Problems, and to introduce a new one, namely “Giving the AI safe tools”.</description>
    </item>
    
    <item>
      <title>Working Notes - Learning PyTorch - Day 2</title>
      <link>https://adamjermyn.com/posts/ml_2/</link>
      <pubDate>Tue, 12 Apr 2022 17:17:54 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/ml_2/</guid>
      <description>Summary  Adding convolutional layers is pretty straightforward, but there are some indexing subtleties to watch. Pooling layers can really improve performance. With two pooling layers, I found an example of a too-high learning rate making the model get stuck with poor performance. The more pooling layers I used the faster the model took a training step (because the dense linear layer was smaller). The more pooling layers I used, the more important the number of channels in the convolutional/pooling layers became.</description>
    </item>
    
    <item>
      <title>Working Notes - Learning PyTorch - Day 1</title>
      <link>https://adamjermyn.com/posts/ml_1/</link>
      <pubDate>Mon, 11 Apr 2022 17:06:39 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/ml_1/</guid>
      <description>Summary  Covered boilerplate for getting started, tensors, gradients. Poked in some detail at how gradients work. Played with linear regression, nonlinear regression, both with gradient descent. Built an MNIST classifier following a PyTorch tutorial. Experimented with the shape and size of the NN. Got PyTorch running on a machine with a GPU.  Boilerplate Import:
import torch Need to use a special float type for the elements of tensors, and need to know which device the compute graph will run on:</description>
    </item>
    
  </channel>
</rss>
