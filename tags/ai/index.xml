<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on Adam Jermyn</title>
    <link>https://adamjermyn.com/tags/ai/</link>
    <description>Recent content in AI on Adam Jermyn</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 15 Jun 2022 15:33:51 -0400</lastBuildDate><atom:link href="https://adamjermyn.com/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Multigate Priors</title>
      <link>https://adamjermyn.com/posts/multigate_prior/</link>
      <pubDate>Wed, 15 Jun 2022 15:33:51 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/multigate_prior/</guid>
      <description>Posted to LessWrong here.
(Thanks to Evan Hubinger and Nicholas Schiefer for suggestions and discussions around these ideas)
Multi-Gate Traces We can improve on one-gate traces with multi-gate traces! Suppose we have $N$ training samples. Then we can reasonably estimate joint probability distributions over up to $k=\lfloor\log_2 N\rfloor$ gates, giving the prior:
$$ p\propto \exp\left(-\sum_{i_1&amp;hellip;i_k}\sum_{j\in [0,1]^k}p_{i_1&amp;hellip;i_k}(j)^m\right) $$
That is, we sum over all selections of $k$ gates and compute the entropy of their joint distribution over $2^k$ states.</description>
    </item>
    
    <item>
      <title>Trace Training Priors</title>
      <link>https://adamjermyn.com/posts/trace_training/</link>
      <pubDate>Mon, 13 Jun 2022 13:14:01 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/trace_training/</guid>
      <description>(Crossposted from the AI Alignment Forums)
I’m worried about scenarios involving deceptive models. We’ve failed at inner alignment so the model has goals that are not aligned with ours. It can somehow detect when it’s in training, and during training it pretends to share our goals. During deployment, surprise! The model paperclips the universe.
In this story deception is all about the model having hidden behaviors that never get triggered during training.</description>
    </item>
    
    <item>
      <title>ELK Proposal - Make the Reporter care about the Predictor’s beliefs</title>
      <link>https://adamjermyn.com/posts/elk1/</link>
      <pubDate>Mon, 13 Jun 2022 09:27:54 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/elk1/</guid>
      <description>Posted to the AI Alignment Forum here.
(This proposal received an honorable mention in the ELK prize results, and we believe was classified among strategies which “reward reporters that are sensitive to what’s actually happening in the world”. We do not think that the counterexample to that class of strategies works against our proposal, though, and we have explained why in a note at the end. Feedback, disagreement, and new failure modes are very welcome!</description>
    </item>
    
    <item>
      <title>Give the model a model-builder</title>
      <link>https://adamjermyn.com/posts/give_model_builder/</link>
      <pubDate>Mon, 06 Jun 2022 16:54:20 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/give_model_builder/</guid>
      <description>(Thanks to Evan Hubinger for suggesting this problem and for comments on these ideas. Feedback is very welcome. Cross-posted from LessWrong.)
The Setup Suppose we have a prior $p_{\rm good}(\mathrm{model})$ that we think is non-deceptive, such that we can sample from this prior and get good models that are inner-aligned.
These models may, for instrumental purposes, need to themselves produce models (via searches/optimization/meta-learning). For instance we might ask an outer model to play poker.</description>
    </item>
    
    <item>
      <title>Give the AI safe tools</title>
      <link>https://adamjermyn.com/posts/give_ai_tools/</link>
      <pubDate>Sat, 04 Jun 2022 08:21:00 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/give_ai_tools/</guid>
      <description>(Cross-posted from LessWrong)
One kind of concern with AI is that:
 There are some tools that are instrumentally useful for an AI to have. Most/the most accessible versions of those tools are dangerous. The AI doesn’t care which versions are dangerous. Hence, the AI will probably develop dangerous tools for instrumental reasons.  You might call concerns like this Instrumental Danger Problems. This post aims to examine some existing approaches to Instrumental Danger Problems, and to introduce a new one, namely “Giving the AI safe tools”.</description>
    </item>
    
    <item>
      <title>Working Notes - Learning PyTorch - Day 2</title>
      <link>https://adamjermyn.com/posts/ml_2/</link>
      <pubDate>Tue, 12 Apr 2022 17:17:54 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/ml_2/</guid>
      <description>Summary  Adding convolutional layers is pretty straightforward, but there are some indexing subtleties to watch. Pooling layers can really improve performance. With two pooling layers, I found an example of a too-high learning rate making the model get stuck with poor performance. The more pooling layers I used the faster the model took a training step (because the dense linear layer was smaller). The more pooling layers I used, the more important the number of channels in the convolutional/pooling layers became.</description>
    </item>
    
    <item>
      <title>Working Notes - Learning PyTorch - Day 1</title>
      <link>https://adamjermyn.com/posts/ml_1/</link>
      <pubDate>Mon, 11 Apr 2022 17:06:39 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/ml_1/</guid>
      <description>Summary  Covered boilerplate for getting started, tensors, gradients. Poked in some detail at how gradients work. Played with linear regression, nonlinear regression, both with gradient descent. Built an MNIST classifier following a PyTorch tutorial. Experimented with the shape and size of the NN. Got PyTorch running on a machine with a GPU.  Boilerplate Import:
import torch Need to use a special float type for the elements of tensors, and need to know which device the compute graph will run on:</description>
    </item>
    
  </channel>
</rss>
