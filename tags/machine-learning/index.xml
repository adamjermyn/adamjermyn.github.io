<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine-Learning on Adam Jermyn</title>
    <link>https://adamjermyn.com/tags/machine-learning/</link>
    <description>Recent content in Machine-Learning on Adam Jermyn</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 11 May 2022 15:28:14 -0400</lastBuildDate><atom:link href="https://adamjermyn.com/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Automatic Differentiation in MESA</title>
      <link>https://adamjermyn.com/posts/auto_diff_mesa/</link>
      <pubDate>Wed, 11 May 2022 15:28:14 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/auto_diff_mesa/</guid>
      <description>Over the last two years I developed an automatic differentiation (auto_diff) module in Fortran to support development of the Modules for Experiments in Stellar Astrophysics (MESA) project. This post gives a brief rundown of how the auto_diff module works and how I built it.
All of the code I mention below lives on GitHub.
What is auto_diff? Forward-mode automatic differentiation via operator overloading:
 Forward-mode means we calculate the chain rule as we go.</description>
    </item>
    
    <item>
      <title>Working Notes - Learning PyTorch - Day 2</title>
      <link>https://adamjermyn.com/posts/ml_2/</link>
      <pubDate>Tue, 12 Apr 2022 17:17:54 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/ml_2/</guid>
      <description>Summary  Adding convolutional layers is pretty straightforward, but there are some indexing subtleties to watch. Pooling layers can really improve performance. With two pooling layers, I found an example of a too-high learning rate making the model get stuck with poor performance. The more pooling layers I used the faster the model took a training step (because the dense linear layer was smaller). The more pooling layers I used, the more important the number of channels in the convolutional/pooling layers became.</description>
    </item>
    
    <item>
      <title>Working Notes - Learning PyTorch - Day 1</title>
      <link>https://adamjermyn.com/posts/ml_1/</link>
      <pubDate>Mon, 11 Apr 2022 17:06:39 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/ml_1/</guid>
      <description>Summary  Covered boilerplate for getting started, tensors, gradients. Poked in some detail at how gradients work. Played with linear regression, nonlinear regression, both with gradient descent. Built an MNIST classifier following a PyTorch tutorial. Experimented with the shape and size of the NN. Got PyTorch running on a machine with a GPU.  Boilerplate Import:
import torch Need to use a special float type for the elements of tensors, and need to know which device the compute graph will run on:</description>
    </item>
    
  </channel>
</rss>
