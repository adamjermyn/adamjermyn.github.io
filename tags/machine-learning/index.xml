<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine-Learning on Adam Jermyn</title>
    <link>https://adamjermyn.com/tags/machine-learning/</link>
    <description>Recent content in Machine-Learning on Adam Jermyn</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 26 Sep 2022 08:21:00 -0400</lastBuildDate><atom:link href="https://adamjermyn.com/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Brief Notes on Transformers</title>
      <link>https://adamjermyn.com/posts/transformer_notes/</link>
      <pubDate>Mon, 26 Sep 2022 08:21:00 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/transformer_notes/</guid>
      <description>(Cross-posted from the AI Alignment Forum)*
*These are just some notes I wrote while reading about transformers, which I thought might be a useful reference to others. Corrections welcome.
Overview of Transformers Many transformer models have the following architecture:
Data flows as follows:
 We take tokens as inputs and pass them through an embedding layer. The embedding layer outputs its result into the residual stream (x0). This has dimension (C,E), where C is the number of tokens in the context window and E is the embedding dimension.</description>
    </item>
    
    <item>
      <title>Automatic Differentiation in MESA</title>
      <link>https://adamjermyn.com/posts/auto_diff_mesa/</link>
      <pubDate>Wed, 11 May 2022 15:28:14 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/auto_diff_mesa/</guid>
      <description>Over the last two years I developed an automatic differentiation (auto_diff) module in Fortran to support development of the Modules for Experiments in Stellar Astrophysics (MESA) project. This post gives a brief rundown of how the auto_diff module works and how I built it.
All of the code I mention below lives on GitHub.
What is auto_diff? Forward-mode automatic differentiation via operator overloading:
 Forward-mode means we calculate the chain rule as we go.</description>
    </item>
    
    <item>
      <title>Working Notes - Learning PyTorch - Day 2</title>
      <link>https://adamjermyn.com/posts/ml_2/</link>
      <pubDate>Tue, 12 Apr 2022 17:17:54 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/ml_2/</guid>
      <description>Summary  Adding convolutional layers is pretty straightforward, but there are some indexing subtleties to watch. Pooling layers can really improve performance. With two pooling layers, I found an example of a too-high learning rate making the model get stuck with poor performance. The more pooling layers I used the faster the model took a training step (because the dense linear layer was smaller). The more pooling layers I used, the more important the number of channels in the convolutional/pooling layers became.</description>
    </item>
    
    <item>
      <title>Working Notes - Learning PyTorch - Day 1</title>
      <link>https://adamjermyn.com/posts/ml_1/</link>
      <pubDate>Mon, 11 Apr 2022 17:06:39 -0400</pubDate>
      
      <guid>https://adamjermyn.com/posts/ml_1/</guid>
      <description>Summary  Covered boilerplate for getting started, tensors, gradients. Poked in some detail at how gradients work. Played with linear regression, nonlinear regression, both with gradient descent. Built an MNIST classifier following a PyTorch tutorial. Experimented with the shape and size of the NN. Got PyTorch running on a machine with a GPU.  Boilerplate Import:
import torch Need to use a special float type for the elements of tensors, and need to know which device the compute graph will run on:</description>
    </item>
    
  </channel>
</rss>
